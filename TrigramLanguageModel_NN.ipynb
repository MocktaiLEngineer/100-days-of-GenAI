{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MocktaiLEngineer/100-days-of-GenAI/blob/main/TrigramLanguageModel_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "pSOlVjuGa9Qv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krgQ4l9vYEd9",
        "outputId": "14801b89-f39d-406b-bc8b-11c5c872e48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-25 10:49:37--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-03-25 10:49:37 (22.2 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the names.txt file\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt','r').read().splitlines()"
      ],
      "metadata": {
        "id": "0Lr-NlWVYbII"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN = '.'\n",
        "vocab = [TOKEN] + sorted(list(set(''.join(words)))) "
      ],
      "metadata": {
        "id": "rmS__lj6Zeik"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(vocab)"
      ],
      "metadata": {
        "id": "FIBSlO3KgcES"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_int = {char:i for i,char in enumerate(vocab)}\n",
        "int_to_char = {i:char for char,i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "xXYltqmHZrYT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset of bigrams (x,y)\n",
        "\n",
        "X,Y = [],[]\n",
        "\n",
        "for word in words:\n",
        "    word = [TOKEN] + list(word) + [TOKEN]\n",
        "    for ch1,ch2,ch3 in zip(word,word[1:],word[2:]):\n",
        "        ix1 = char_to_int[ch1]\n",
        "        ix2 = char_to_int[ch2]\n",
        "        ix3 = char_to_int[ch3]\n",
        "        X.append([ix1,ix2])\n",
        "        Y.append(ix3)\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "print(X,Y)\n",
        "\n",
        "# Initialising the network\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "W = torch.randn((n * 2,n), requires_grad = True, generator=g)\n",
        "num = X.shape[0]"
      ],
      "metadata": {
        "id": "7XIzsjrzYQDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dcfdd2-bf8f-4f3a-dc2a-1b8b36089bcd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  5],\n",
            "        [ 5, 13],\n",
            "        [13, 13],\n",
            "        ...,\n",
            "        [26, 25],\n",
            "        [25, 26],\n",
            "        [26, 24]]) tensor([13, 13,  1,  ..., 26, 24,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(200):\n",
        "    # Forward pass\n",
        "    xenc = F.one_hot(X, num_classes = n).float()\n",
        "\n",
        "    xenc_flattened = xenc.view(xenc.shape[0],-1)\n",
        "\n",
        "    logits = xenc_flattened @ W #Log counts\n",
        "    counts = logits.exp()\n",
        "    probs = counts / torch.sum(counts, dim = 1, keepdim = True)\n",
        "    loss = -probs[torch.arange(num), Y].log().mean() \n",
        "\n",
        "    # Backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    W.data += -50 * W.grad\n",
        "\n",
        "    print(f\"Iteration:{i} | {loss=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkimpiPg8WwJ",
        "outputId": "733653cf-98f7-41e0-a6b2-918d9fa08ecd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:0 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:1 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:2 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:3 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:4 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:5 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:6 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:7 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:8 | loss=tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Iteration:9 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:10 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:11 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:12 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:13 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:14 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:15 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:16 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:17 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:18 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:19 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:20 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:21 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:22 | loss=tensor(2.2398, grad_fn=<NegBackward0>)\n",
            "Iteration:23 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:24 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:25 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:26 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:27 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:28 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:29 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:30 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:31 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:32 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:33 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:34 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:35 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:36 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:37 | loss=tensor(2.2397, grad_fn=<NegBackward0>)\n",
            "Iteration:38 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:39 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:40 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:41 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:42 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:43 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:44 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:45 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:46 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:47 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:48 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:49 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:50 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:51 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:52 | loss=tensor(2.2396, grad_fn=<NegBackward0>)\n",
            "Iteration:53 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:54 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:55 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:56 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:57 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:58 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:59 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:60 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:61 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:62 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:63 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:64 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:65 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:66 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:67 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:68 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:69 | loss=tensor(2.2395, grad_fn=<NegBackward0>)\n",
            "Iteration:70 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:71 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:72 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:73 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:74 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:75 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:76 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:77 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:78 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:79 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:80 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:81 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:82 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:83 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:84 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:85 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:86 | loss=tensor(2.2394, grad_fn=<NegBackward0>)\n",
            "Iteration:87 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:88 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:89 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:90 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:91 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:92 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:93 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:94 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:95 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:96 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:97 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:98 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:99 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:100 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:101 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:102 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:103 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:104 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:105 | loss=tensor(2.2393, grad_fn=<NegBackward0>)\n",
            "Iteration:106 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:107 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:108 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:109 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:110 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:111 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:112 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:113 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:114 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:115 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:116 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:117 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:118 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:119 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:120 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:121 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:122 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:123 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:124 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:125 | loss=tensor(2.2392, grad_fn=<NegBackward0>)\n",
            "Iteration:126 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:127 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:128 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:129 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:130 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:131 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:132 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:133 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:134 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:135 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:136 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:137 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:138 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:139 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:140 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:141 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:142 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:143 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:144 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:145 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:146 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:147 | loss=tensor(2.2391, grad_fn=<NegBackward0>)\n",
            "Iteration:148 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:149 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:150 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:151 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:152 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:153 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:154 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:155 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:156 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:157 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:158 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:159 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:160 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:161 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:162 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:163 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:164 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:165 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:166 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:167 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:168 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:169 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:170 | loss=tensor(2.2390, grad_fn=<NegBackward0>)\n",
            "Iteration:171 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:172 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:173 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:174 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:175 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:176 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:177 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:178 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:179 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:180 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:181 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:182 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:183 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:184 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:185 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:186 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:187 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:188 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:189 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:190 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:191 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:192 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:193 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:194 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:195 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:196 | loss=tensor(2.2389, grad_fn=<NegBackward0>)\n",
            "Iteration:197 | loss=tensor(2.2388, grad_fn=<NegBackward0>)\n",
            "Iteration:198 | loss=tensor(2.2388, grad_fn=<NegBackward0>)\n",
            "Iteration:199 | loss=tensor(2.2388, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's sample and generate\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "for i in range(10):\n",
        "  name = []\n",
        "  \n",
        "  ix1 = 0\n",
        "  ix2 = 0\n",
        "\n",
        "  while True:\n",
        "    xenc = F.one_hot(torch.tensor([ix1,ix2]), num_classes = n).float()\n",
        "    \n",
        "    xenc_flattened = xenc.view(1, -1)\n",
        "    \n",
        "    logits = xenc_flattened @ W #Log counts\n",
        "\n",
        "\n",
        "    counts = logits.exp()\n",
        "    probs = counts / torch.sum(counts, dim = 1, keepdim = True)\n",
        "    ix2 = torch.multinomial(probs, num_samples = 1, replacement = True, generator = g).item()\n",
        "    name.append(int_to_char[ix2])\n",
        "\n",
        "    ix1 = ix2\n",
        "\n",
        "    if ix2 == 0:\n",
        "      break\n",
        "  print(''.join(name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STG63E8d8GXX",
        "outputId": "1241e434-0791-4b73-eff1-29ae126bbea0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nin.\n",
            "avo.\n",
            "militlynyleralumin.\n",
            "arada.\n",
            "nalyhizaridan.\n",
            "an.\n",
            "kegade.\n",
            "ri.\n",
            "adenemeshan.\n",
            "adynar.\n"
          ]
        }
      ]
    }
  ]
}